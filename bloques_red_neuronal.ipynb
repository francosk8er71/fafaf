{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloques de una red neuronal\n",
    "\n",
    "Este notebook tiene como objetivo proporcionar intuiciones sobre las distintas nociones que son necesarias para entender las redes neuronales, pero sin resultar excesivamente técnicos. En concreto, se intentará evitar en la medida de lo posible la notación matemática, que no es estrictamente necesaria para entender cómo funcionan las redes neuronales. \n",
    "\n",
    "Para comenzar, empezaremos con un ejemplo práctico de una red neuronal, y a continuación iremos explicando los conceptos que van apareciendo. Este notebook está basado en el libro \"Deep Learning with Python\" de F. Chollet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un primer vistazo a una red neuronal\n",
    "\n",
    "Para este ejemplo vamos a utilizar de nuevo el dataset de los dígitos (MNIST) que ya hemos visto en prácticas anteriores. El problema que se intenta resolver con este dataset consiste en clasificar imágenes en escala de grises de dígitos manuscritos (de tamaño 28x28) en 10 categorías (del 0 al 9). Este dataset se considera como el 'Hello world' del aprendizaje supervisado y se utiliza de manera habitual para probar nuevos algoritmos. \n",
    "\n",
    "Para este ejemplo es necesario que instales la librería ```keras```, una librería de aprendizaje profundo de la cual nosotros utilizaremos únicamente la parte de redes neuronales. Puedes instalar esta librería mediante pip usando ```pip install keras```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando el dataset MNIST\n",
    "\n",
    "El dataset MNIST viene precargado en Keras en la forma de 4 vectores de Numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11460608/11490434 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(imagenes_entrenamiento,etiquetas_entrenamiento),(imagenes_test,etiquetas_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(imagenes_entrenamiento))\n",
    "print(len(imagenes_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio\n",
    "Las imágenes se codifican como arrays de Numpy, y las etiquetas como un array de dígitos. ¿Cuántas imágenes hay de entrenamiento? ¿y de test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "60000 de entrenamiento y 10000 de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a entrenar el modelo. El proceso será el siguiente. Comenzaremos alimentando a la red neuronal con los datos de entrenamiento (almacenados en ```imagenes_entrenamiento``` y ```etiquetas_entrenamiento```), de este modo la red aprenderá a asociar imágenes con etiquetas. Finalmente, pediremos a la red que produzca predicciones para las imágenes de test y verificaremos que los resultados cuadran con las etiquetas previstas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura de la red\n",
    "\n",
    "Vamos a construir una red con tres capas. La primera capa o capa de entrada consta de 784 nodos (recordar que las imágenes son de tamaño 28x28 que es 784), esto quiere decir que nuestro vector de descriptores para cada imagen es la imagen aplanada. La primera capa está completamente conectada a una segunda capa con 512 nodos. Para esta capa intermedia, la función de activación de los nodos será la función relu. La tercera capa (que es la capa final) tiene 10 nodos con una función de activación softmax. Esto significa que la red devolverá un vector con 10 valores. Cada uno de estos valores indica la probabilidad de que la imagen pasada a la red pertenezca a cada una de las 10 clases.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512,activation='relu',input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilación de la red\n",
    "\n",
    "Para que la red esté lista para entrenarse debemos configurar tres parámetros adicionales antes del proceso de entrenamiento. \n",
    "\n",
    "- Una _función de pérdida_: esta función nos indica cómo la red mide su rendimiento con los datos de entrenamiento, y por lo tanto si los cambios que se producen en los pesos de la red van en la dirección adecuada.\n",
    "- Un _optimizador_: el mecanismo que actualiza los pesos de la red. \n",
    "- Una _métrica_ a monitorizar durante el proceso de entrenamiento y test. \n",
    "\n",
    "Al proceso de fijar estos tres parámetros se le conoce en Keras como _compilar_ la red. A continuación vemos un ejemplo de cómo compilar la red. La explicación de los distintos parámetros se explica más adelante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando los datos\n",
    "\n",
    "Ya hemos comentado en clase que en muchos casos es necesario preprocesar los datos antes de entrenar la red. En este caso vamos a reestructura nuestros datos y escalar los valores en el intervalo $[0,1]$. \n",
    "\n",
    "En concreto, el array imagenes_entrenamiento es un array con la forma (60000,28,28) (esto lo puedes ver con la instrucción ```imagenes_entrenamiento.shape```) de elementos enteros en el intervalo $[0,255]$. Que tenga esta forma significa que nuestro dataset consta de 60000 instancias, y que cada instancia viene dada por una matriz de tamaño 28x28 con valores en el intervalo $[0,255]$. Vamos a reestructurar nuestros datos para que tengan la forma (60000,28x28) con valores reales entre 0 y 1. Es decir, tendremos 60000 instancias, cada una de ellas representada por un vector de tamaño 784 (28x28) de reales entre 0 y 1. Este tipo de transformaciones son comunes a la hora de entrenar una red neuronal.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenes_entrenamiento = imagenes_entrenamiento.reshape((60000,28*28))\n",
    "imagenes_entrenamiento = imagenes_entrenamiento.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proceso también es necesario para que las imágenes de test tengan el mismo formato. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenes_test = imagenes_test.reshape((10000,28*28))\n",
    "imagenes_test = imagenes_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando las etiquetas\n",
    "También es necesario transformar las etiquetas a una codificación categórica. Esto nos servirá para dar más adelante una probabilidad para cada una de las categorías que puede tener una imagen de nuestro dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "etiquetas_entrenamiento = to_categorical(etiquetas_entrenamiento)\n",
    "etiquetas_test = to_categorical(etiquetas_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "Ya estamos listos para entrenar la red, esto se realiza de igual modo que en la librería ```sklearn```, llamando a la función ```fit```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.2608 - acc: 0.9240     \n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.1039 - acc: 0.9685     \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.0689 - acc: 0.9793     \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.0493 - acc: 0.9852     \n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.0379 - acc: 0.9887     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09be644ef0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(imagenes_entrenamiento,etiquetas_entrenamiento,epochs=5,batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al entrenar la red se muestran dos cantidades: la pérdida de la red sobre los datos de entrenamiento, y la precisión de la red sobre dichos datos. Como podemos ver, la precisión de la red es de 0,989, es decir una precisión del 98,9%. Veámos ahora cuál es la precisión en el conjunto de test utilizando la función ```evaluate```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0stest_acc: 0.9808\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc = network.evaluate(imagenes_test,etiquetas_test)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio\n",
    "¿Qué precisión se obtiene en el conjunto de test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "98.08% de precision en la red con el conjunto de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto concluye nuestro ejemplo de red neuronal. Ahora vamos a ver el detalle de cada una de las componentes de esta red. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación de datos para las redes neuronales\n",
    "\n",
    "En el ejemplo previo hemos usado un vector multidimensional de Numpy para almacenar nuestros datos. A dichos vectores se los conoce como **tensores**. En general, la mayoría de sistemas de aprendizaje automático actuales usan los tensores como estructura básica de datos. \n",
    "\n",
    "Los tensores son contenedores de datos, en concreto un contenedor para números. La idea intuitiva de los tensores es que son una generalización de las matrices a un número arbitrario de dimensiones. En concreto una matriz es un tensor de dimensión 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalares (tensores de dimensión 0)\n",
    "Un tensor que sólo contiene un número se llama _escalar_. En Numpy, un número real es un tensor escalar. En Numpy se pueden mostrar los ejes de un tensor utilizando el atributo ```ndim```. En el caso de un tensor escalar, el número de ejes es 0. Veámos un ejemplo. Comenzamos importando numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un escalar y lo almacenamos en la variable x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos dicha variable y a continuación su dimensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectores (tensores de dimensión 1)\n",
    "Un vector de números es un tensor de dimensión 1. Un tensor de dimensión 1 tiene un único eje. El siguiente código muestra un vector de Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([12,3,6,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  3,  6, 14])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices (tensores de dimensión 2)\n",
    "Un vector de vectores es una matriz, también llamada tensor de dimensión 2. Una matriz tiene 2 ejes, las filas y columnas. A continuación se da un ejemplo de matriz en Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[5,78,2,34,0],\n",
    "              [6,79,3,35,1],\n",
    "              [7,80,4,36,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 78,  2, 34,  0],\n",
       "       [ 6, 79,  3, 35,  1],\n",
       "       [ 7, 80,  4, 36,  2]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensores de dimensión 3 y superior\n",
    "Si empaquetamos matrices en un nuevo vector se obtiene un tensor de dimensión 3, que puede verse como un cubo de números. Si se empaqueta un tensor de dimensión 3 se obtiene un tensor de dimensión 4 y así sucesivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[ 5, 78,  2, 34,  0],\n",
    "               [ 6, 79,  3, 35,  1],\n",
    "               [ 7, 80,  4, 36,  2]],\n",
    "             [[ 5, 78,  2, 34,  0],\n",
    "               [ 6, 79,  3, 35,  1],\n",
    "               [ 7, 80,  4, 36,  2]],\n",
    "             [[ 5, 78,  2, 34,  0],\n",
    "               [ 6, 79,  3, 35,  1],\n",
    "               [ 7, 80,  4, 36,  2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atributos clave de los tensores\n",
    "Los tensores tienen tres atributos clave:\n",
    "- Número de ejes: por ejemplo, un tensor de dimensión 3 tiene 3 ejes. A este atributo se accede utilizando el atributo ```ndim``` como hemos visto anteriormente.\n",
    "- Forma (en inglés _shape_): una tupla de enteros que describe cuantas dimensiones tiene el tensor en cada eje. Por ejemplo, la matriz anterior tiene forma (3,5), mientras que el tensor de dimensión 3 tiene forma (3,3,5). Un vector como en anterior tiene forma (4,), y un escalar tiene la forma vacía (). Para acceder a la forma de un tensor se utiliza el atributo ```shape```. \n",
    "- El tipo de los datos: normalmente los datos de un tensor tienen tipo entero (tipo ```uint8``` en Python) o real (tipo ```float32``` o ```float64``` en Python). Al tipo de datos de un tensor se accede mediante el atributo ```dtype```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio \n",
    "Los datos almacenados en las variables imagenes_entrenamiento e imagenes_test son tensores. ¿Cuál es el número de ejes, forma y tipo de los datos de estos tensores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "(60000, 784)\n",
      "(10000, 784)\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(imagenes_entrenamiento.ndim)\n",
    "print(imagenes_test.ndim)\n",
    "\n",
    "print(imagenes_entrenamiento.shape)\n",
    "print(imagenes_test.shape)\n",
    "\n",
    "print(imagenes_entrenamiento.dtype)\n",
    "print(imagenes_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulando tensores en Numpy\n",
    "\n",
    "Del mismo modo que hemos visto cómo rebanar strings y listas también se pueden rebanar tensores. El siguiente ejemplo toma el 4 elemento de las imágenes de entrenamiento, le cambia la forma (recordar que habíamos pasado de representar cada imagen como una matriz a representarlo como un vector) y lo muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "digito = imagenes_entrenamiento[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "digito = digito.reshape(28,28)\n",
    "digito = (digito * 255).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digito,cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código toma las imágenes de la 10 a la 100 (sin incluir la 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corte = imagenes_entrenamiento[10:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 784)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corte.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La noción de lotes de datos (_batches_)\n",
    "\n",
    "Cada vez más a menudo, los datasets son tan sumamente grandes que no es factible mostrar todo el dataset al algoritmo que se está entrenando. Para paliar este problema se utilizan lo que se conocen como _lotes de datos_ o _batches_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos reales de tensores\n",
    "La mayoría de datos con los que se trabajan en la actualidad encajan en uno de los siguientes tipos de tensores:\n",
    "- Datos vectoriales: tensores 2D con la forma (muestras, descriptores).\n",
    "- Series de datos o series temporales: tensores 3D con la forma (muestras, pasos, descriptores).\n",
    "- Imágenes: tensores 4D con la forma (muestras, filas, columnas, canales).\n",
    "- Vídeos: tensores 5D con la forma (muestras, frames, filas, columnas, canales). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones sobre tensores\n",
    "\n",
    "Todas las operaciones que se producen en una red neuronal se pueden reducir a un pequeño conjunto de operaciones sobre tensores. \n",
    "\n",
    "En nuestro ejemplo inicial, estabamos construyendo una red apilando capas. Una capa de Keras tiene la forma:\n",
    "\n",
    "```python\n",
    "keras.layers.dense(512,activation='relu',input_shape=(28*28,))\n",
    "```\n",
    "\n",
    "Esta capa se puede interpretar como una función que toma como entrada un tensor 2D con la forma (n_samples, 28*28) y devuelve otro tensor 2D con la forma (n_samples,512). En concreto, se tiene una función como la siguiente:\n",
    "\n",
    "```python\n",
    "output= relu(dot(W,input)+b)\n",
    "```\n",
    "\n",
    "En dicha función hay tres operaciones sobre tensores: un producto escalar (operación ```dot```) entre el tensor de entrada y un tensor llamado W que representa los pesos a aprender; una suma (operador ```+```) entre el tensor 2D resultante y un vector b; y, finalmente, una operación ```relu``` definida como ```relu(x) = max(x,0)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones elemento a elemento\n",
    "\n",
    "Las operaciones ```relu``` y la suma son operaciones elemento a elemento; es decir, operaciones que se aplican de manera independiente a cada entrada en los tensores. Esto significa que este tipo de operadores son altamente paralelizables.\n",
    "\n",
    "En la librería Numpy, estas operaciones están optimizadas utilizando BLAS (Basic Linear Algebra Subprograms), que son rutinas eficientes de manipulación de tensores de bajo nivel, altamente paralelizadas y que están implementadas de manera habitual en C o Fortran. \n",
    "\n",
    "Así que en Numpy es muy rápido y sencillo realizar operaciones elemento a elemetno como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([[1,-2,3],[-4,5,-6],[7,-8,9]])\n",
    "y1 = np.array([[1,1,1],[1,1,1],[1,1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suma elemento a elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, -1,  4],\n",
       "       [-3,  6, -5],\n",
       "       [ 8, -7, 10]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1+y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu elemento a elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 3.],\n",
       "       [0., 5., 0.],\n",
       "       [7., 0., 9.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(x1,0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "En un primer vistazo podemos pensar que una operación elemento a elemento que toma dos tensores (como la adición) fuerza a que ambos tensores tengan la misma forma. Sin embargo, hemos dicho anteriormente que estamos sumando un tensor 2D con un vector (tensor 1D). ¿Qué ocurre cuando la forma de los dos tensores a sumar es diferente?\n",
    "\n",
    "Para resolver este problema, y siempre que no haya ambigüedad, el tensor pequeño se amplia (proceso conocido en inglés como _broadcasting_) para que tenga el tamaño del grande. Este es un proceso de dos pasos:\n",
    "- Se añaden ejes al tensor pequeño para llegar al valor de ```ndim``` del tensor grande. \n",
    "- El tensor pequeño se repite a lo largo de estos ejes para llegar a completar la forma completa del tensor grande. \n",
    "\n",
    "Por ejemplo, veamos lo que ocurre si intentamos sumar un tensor 2D con un vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, -2,  3],\n",
       "       [-4,  5, -6],\n",
       "       [ 7, -8,  9]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = [1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, -2,  3],\n",
       "       [-3,  5, -6],\n",
       "       [ 8, -8,  9]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1+y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio\n",
    "¿Cómo se ha ampliado el vector ```y2```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "se amplia hasta alcanzar la ndim de x1, de esta forma\n",
    "y1 -> [[1,0,0],[1,0,0],[1,0,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El producto tensorial\n",
    "\n",
    "El último ingrediente que necesitamos es el _producto tensorial_ (```dot```), que es la operación tensorial más común. Al contrario que las operaciones elemento a elemento, el producto tensorial combina las entradas de los tensores considerados. El producto tensorial entre dos matrices es simplemente el producto de matrices. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  2,  2],\n",
       "       [-5, -5, -5],\n",
       "       [ 8,  8,  8]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x1,y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La optimazión basada en el gradiente\n",
    "\n",
    "Como hemos visto, cada capa de la red neuronal que definimos al principio sirve para transformar sus datos de entrada utilizando la expresión:\n",
    "\n",
    "```python\n",
    "output=relu(dot(W,input)+b)\n",
    "```\n",
    "\n",
    "En dicha expresión, W y b son tensores que representan atributos de la capa. Estos tensores se conocen como pesos de la capa y contienen la información aprendida por la red gracias al entrenamiento. \n",
    "\n",
    "Inicialmente, estos tensores se rellenan con pequeños valores aleatorios, y dichos valores se van ajustando de manera gradual gracias al feedback producido por la red. Este ajuste gradual es lo que se conoce como entrenamiento. \n",
    "\n",
    "Dicho ajuste ocurre en lo que se conoce como bucle de entrenamiento que consta de los siguientes pasos:\n",
    "1. Coger un lote de ejemplos de entrenamiento x y sus correspondientes objetivos y.\n",
    "2. Pasar los datos de entrenamiento a través de la red (en un paso que se conoce como _paso hacia adelante_  o _forward pass_) y obtener predicciones y_pred. \n",
    "3. Calcular la pérdida de la red en el lote y medir el desfase entre y e y_pred.\n",
    "4. Actualizar los pesos de la red de manera que se reduzca poco a poco la pérdida en el lote. \n",
    "\n",
    "De este odo se consigue una red que tiene una pérdida pequeña en los datos de entrenamiento. La red aprende de este modo una función que asigna a las entradas la salida correcta. Esto que parece magia no lo es tanto cuando se mira en detalle cada paso. \n",
    "\n",
    "El paso 1 es simplemente una operación de rebanado. Los pasos 2 y 3 consiste en aplicar una serie de operaciones tensoriales. La única parte difícil es el paso 4: actualizar los pesos de la red. Para resolver este paso se utilizan derivadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué es una derivada?\n",
    "\n",
    "Considerar una función continua y suave $f(x)=y$ que asigna a un número real $x$ un nuevo número real $y$. Debido a que la función es continua, un pequeño cambio en $x$ produce un pequeño cambio en $y$. Digamos que incrementamos $x$ por un pequeño factor $\\epsilon_x$, esto produce un pequeño $\\epsilon_y$ que cambia $y$:\n",
    "$$f(x+\\epsilon_x)=y+\\epsilon_y$$\n",
    "\n",
    "Debido a que la función es suave (es decir que no hay ángulos abruptos), cuando $\\epsilon_x$ es lo suficientemente pequeño cerca de un punto $p$ es posible aproximar $f$ como una función linear de pendiente $a$ de manera que $\\epsilon_y$ se convierte en $a \\times \\epsilon_x$:\n",
    "$$f(x+\\epsilon_x)=y+a\\times \\epsilon_x$$\n",
    "\n",
    "Esta aproximación sólo es válida cuando $x$ está lo suficientemente cerca de $p$. La pendiente $a$ es lo que se conoce como _derivada_ de $f$ en $p$. Si $a$ es negativa, significa que un pequeño cambio positivo de $x$ cerca de $p$ supone que $f(x)$ disminuye; en cambio, si $a$ es positivo, significa que un pequeño cambio positivo de $x$ cerca de $p$ supone que $f(x)$ aumenta. Esto se puede ver en la siguiente imagen.\n",
    "\n",
    "<img src=\"images/derivada.png\">\n",
    "\n",
    "Además el valor absoluto de $a$ nos indica cómo de rápido sucede dicho incremento o decremento. \n",
    "\n",
    "Para toda función diferenciable $f(x)$ (donde diferenciable significa que puede ser derivada), existe una función derivada $f'(x)$ que asigna valores de $x$ a la pendiente de la aproximación local lineal de $f$ en esos puntos. Por ejemplo, la deriva del $cos(x)$ es $-seno(x)$, la derivada de $f(x)=ax$ es $f'(x)=a$.\n",
    "\n",
    "Si estamos intentando actualizar $x$ por un factor $\\epsilon_x$ con el objetivo de minimizar $f$, y se conoce la derivada de $f$, entonces la derivada de $f$ describe de manera completa como evolucion $f(x)$ al cambiar $x$. Si queremos reducir el valor de $f(x)$ solo tenemos que mover $x$ un poco en la dirección contraria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El gradiente: la derivada de un tensor\n",
    "\n",
    "El _gradiente_ es la derivada de una operación tensorial; en concreto, el gradiente es la generalización del concepto de derivadas a funciones con múltiples entradas.\n",
    "\n",
    "Considerar un vector de entrada ```x```, una matriz ```W```, un objetivo ```y``` y una función de pérdida ```loss```. Se puede usar ```W``` para calcular un candidato ```y_pred``` y calcular la pérdida entre el resultado obtenido ```y_pred``` y el resultado esperado ```y```:\n",
    "\n",
    "```python\n",
    "y_pred = dot(W,x)\n",
    "valor_perdida = loss(y_pred,y)\n",
    "```\n",
    "\n",
    "Si los datos de entrada ```x``` e ```y``` son fijos, el código anterior se puede interpretar como una función que asigna valores de ```W``` a valores de pérdida:\n",
    "\n",
    "```python\n",
    "valor_perdida = f(W)\n",
    "```\n",
    "\n",
    "Supongamos que el valor de ```W``` es inicialmente ```W0```, entonces la derivada de ```f``` en el punto ```W0``` es un tensor ```gradient(f)(W0)``` con la misma forma que ```W```, donde cada coeficiente ```gradient(f)(W0)[i,j]``` indica la dirección y magnitud del cambio en ```valor_perdidad``` cuando modificamos ```W0[i,j]```. El tensor ```gradient(f)(W0)```es el gradiente de la función ```f(W)``` en ```W0```. \n",
    "\n",
    "Hemos visto anteriormente que el valor de una derivada de una función ```f(x)``` con un único coeficiente pude interpretarse como la pendiente de la curva de ```f```. Del mismo modo ```gradient(f)(W0)``` puede interpretarse como un tensor que describe la _curvatura_ de ```f(W)``` alrededor de ```W0```. \n",
    "\n",
    "Es por esto que del mismo modo que pra una función ```f(x)``` se puede reducir el valor de ```f(x)``` moviendose un poco en la dirección opuesta a la derivada; con una función ```f(W)``` de un tensor, se puede reducir ```f(W)``` moviendo ```W``` en la dirección opuesta al gradiente; por ejemplo, ```W1=W0-step*gradient(f)(W0)```. Esto significa ir en contra de la curvatura, que intuitivamente debería ponerte más abajo en la curva. Notar que hay un factor de escala ```step``` que es necesario dado que ```gradient(f)(W0)``` solo aproxima la curvatura cuando estamos cerca de ```W0```, de modo que no nos vayamos demasiado lejos de ```W0```. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descenso de gradiente estocástico\n",
    "\n",
    "Dada una función diferenciable, es posible encontrar, al menos de manera teórica, su mínimo de manera analítica: se sabe que el mínimo de una función es un putno donde su derivada es 0, por lo tanto lo único que hay que hacer es encontrar todos los puntos de la función donde la derivada es 0 y comprobar cual de esos puntos tiene el valor más bajo.\n",
    "\n",
    "Aplicado a las redes neuronales, esto significa encontrar de manera analítica la combinación de pesos que produce el menor valor de pérdida. Esto se puede hacer resolviendo la ecuación ```gradient(f)(W)=0``` para ```W```. Esto no es en general posible debido a la cantidad de parámetros (miles e incluso millones) que tienen las redes neuronales actuales. \n",
    "\n",
    "En su lugar, se puede llevar a cabo el algoritmo de cuatro pasos que indicamos al principio de esta sección que basicamente consiste en modificar poco a poco los parámetros basándonos en el valor actual de pérdidad producido en un lote aleatorio de instancias. Dado que estamos trabajando siempre con funciones diferenciables, es posible calcular el gradiente, lo que proporciona un modo eficiente de implementar el paso 4 del algoritmo. Si se actualizan los pesos en la dirección contraria al gradiente, la pérdida será cada vez más pequeña. En concreto el proceso se puede definir con los siguientes pasos:\n",
    "\n",
    "1. Tomar un lote de instancias de entrenamiento ```x``` y sus correspondientes objetivos ```y```.\n",
    "2. Pasar por la red ```x``` para obtener las predicciones ```y_pred```.\n",
    "3. Calcular la pérdida de la red en el lote (medir el error entre ```y_pred``` e ```y```).\n",
    "4. Calcular el gradiente de la pérdida con respecto a los parámetros de la red (un _paso hacia atrás_).\n",
    "5. Mover los parámetros un poco en la dirección opuesta al gradiente de modo que se reduzca la pérdida en el lote un poco. \n",
    "\n",
    "Esto que acabamos de describir se conoce como _descenso de gradiente estocástico en mini-lotes_ o en inglés _mini-batch stochastic gradient descent_ (mini SGD). El término estocástico se refiere al hecho de que cada lote de datos es tomado de manera aleatoria. La siguiente figura ilustra lo que ocurre en dimensión 1 cuando la red sólo tiene un parámetro.\n",
    "\n",
    "<img src=\"images/sgd.png\">\n",
    "\n",
    "Como se puede ver en la figura anterior es necesario elegir un valor razonable para el factor ```step```. En caso de ser muy pequeño el descenso en la curva puede llevar muchas iteraciones, y se podía quedar encallado en un mínimo local. Si el valor de ```step``` es demasiado grande, las actualizaciones pueden llevar a posiciones completamente aleatorias en la curva.\n",
    "\n",
    "Una variante del algoritmo mini SGD consiste en tomar una única instancia en cada iteración en lugar de un lote de instancias. Esto es lo que se conoce como _verdadero SGD_. También se puede ir al otro extremo y coger todo el dataset como lote, a lo que se conoce como _batch SGD_. En el _verdadero SGD_ las actualizaciones serán poco precisas pero serán muy rápidas, por el contrario en _batch SGD_ cada actualización será más precisa pero también mucho más costosa. La solución es tomar un valor razonable de mini-lotes. \n",
    "\n",
    "De manera adicional a las variantes de SGD, existen otras alternativas como Adagrad o RMSProp. A dichas alternativas se las conoce como _métodos de optimización_ u _optimizadores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encadenando derivadas: el algoritmo de propagación hacia atrás\n",
    "\n",
    "En el algoritmo previo hemos asumido que si una función es diferenciable, podemos calcular de manera sencilla su derivada. En la práctica, una función de una red neuronal consiste en muchas operaciones tensoriales encadenadas donde cada una de ellas tiene una derivada simple y conocida.\n",
    "\n",
    "Por ejemplo, considerar la siguiente red compuesta de tres operaciones tensoriales $a$, $b$ y $c$ con matrices de pesos conocidas $W_1$, $W_2$ y $W_3$:\n",
    "$$f(W_1,W_2,W_3)=a(W_1,b(W_2,c(W_3)))$$\n",
    "\n",
    "El Análisis matemático nos dice que tal encadenamiento de funciones puede ser derivado utilizando la regla de la cadena:\n",
    "$$f(g(X))=f'(g(x))\\times g'(x)$$\n",
    "\n",
    "Al aplicar la regla de la cadena al cálculo de los valores del gradiente de una red neuronal se obtiene un algoritmo llamado de __propagación hacia atrás__ (en inglés _backpropagation_). El algoritmo de propagación hacia atrás comienza con el valor de pérdida final y trabaja hacia atrás en las capas superiores y desde las capas inferiores aplicando la regla de la cadena para calcular la contribución que tiene cada parámetro en el cálculo de la perdida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volviendo a nuestro primer ejemplo\n",
    "\n",
    "Ahora podemos revisar lo que hacíamos en nuestro ejemplo inicial comprendiendo las distintas operaciones que estamos realizando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos nuestros datos de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(imagenes_entrenamiento,etiquetas_entrenamiento),(imagenes_test,etiquetas_test) = mnist.load_data()\n",
    "imagenes_entrenamiento = imagenes_entrenamiento.reshape((60000,28*28))\n",
    "imagenes_entrenamiento = imagenes_entrenamiento.astype('float32')/255\n",
    "imagenes_test = imagenes_test.reshape((10000,28*28))\n",
    "imagenes_test = imagenes_test.astype('float32')/255\n",
    "etiquetas_entrenamiento = to_categorical(etiquetas_entrenamiento)\n",
    "etiquetas_test = to_categorical(etiquetas_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se puede comprender que las imágenes de entrada se almacenan en tensores, que tienen la forma (60000,784) para el conjunto de entrenamiento, y (10000,784) para el conjunto de test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra red venía dada del siguiente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512,activation='relu',input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queda claro que la red consta de dos capas enlazadas, que cada capa aplica una serie de operaciones tensoriales a los datos de entrada y que estas operaciones involucran pesos (que son tensores). Los tensores peso, que son atributos de la red, es donde el _conocimiento_ de la red persiste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente que hicimos fue compilar la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto significa que ```categorical_crossentropy``` es la función de pérdida que usamos para dar feedback a la red de manera que los pesos tensoriales aprendan, y que en la fase de entrenamiento se intenta minimizar. Este proceso de reducción de la pérdida ocurre gracias al desceso de gradiente estocástico usando mini-lotes. Las reglas exactas que gobiernan el descenso de gradiente quedan definidas en el optimizador ```rmsprop```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, teníamos el bucle de entrenamiento que se itera 5 veces (cada iteración sobre el conjunto de entrenamiento se conoce como _época_, en inglés _epoch_) utilizando mini-lotes de tamaño 128. En cada iteración, la red calcula los gradientes de los pesos con respecto a la pérdida del lote, y actualiza los pesos de la manera adecuada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.2568 - acc: 0.9253     \n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.1040 - acc: 0.9691     \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.0682 - acc: 0.9801     \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.0496 - acc: 0.9850     \n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s - loss: 0.0374 - acc: 0.9889     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09618975f8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(imagenes_entrenamiento,etiquetas_entrenamiento,epochs=5,batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que hemos visto en este notebook son los conceptos básicos que forman parte de una red neuronal. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
